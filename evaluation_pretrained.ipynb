{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Phi-1.5 on code completion"
      ],
      "metadata": {
        "id": "dzqq7hY6H4ZD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FO15-_u-HeRG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy\n",
        "!pip install python-Levenshtein\n",
        "!pip install tree-sitter\n",
        "!pip install rouge-score"
      ],
      "metadata": {
        "id": "AqARpqyGd9KZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import datetime\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from fuzzywuzzy import fuzz\n",
        "import Levenshtein\n",
        "import nltk\n",
        "from rouge_score import rouge_scorer\n",
        "import tree_sitter"
      ],
      "metadata": {
        "id": "CP7PwGD3H-g_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_default_device(\"cuda\")\n",
        "\n",
        "print(\"Loading model...\")\n",
        "time = datetime.datetime.now()\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", torch_dtype=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n",
        "time1 = datetime.datetime.now()\n",
        "print(f\"Model loaded. Time to load the model: {time1 - time}\")"
      ],
      "metadata": {
        "id": "RE8a8QSzIAXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_tags(code):\n",
        "    \"\"\"\n",
        "    Replaces special tags in the input code with their corresponding literals or empty strings.\n",
        "    Original function is here:\n",
        "    https://github.com/microsoft/CodeXGLUE/blob/main/Code-Code/CodeCompletion-line/evaluator/evaluator.py\n",
        "\n",
        "    Parameters:\n",
        "        code (str): The input code containing special tags.\n",
        "\n",
        "    Returns:\n",
        "        str: The code with tags replaced by literals or empty strings.\n",
        "    \"\"\"\n",
        "    # Replace special tags with their corresponding literals or empty strings\n",
        "    code = code.replace(\"<NUM_LIT>\", \"0\").replace(\"<STR_LIT>\", \"\").replace(\"<CHAR_LIT>\", \"\")\n",
        "\n",
        "    # Find literals enclosed in special tags and replace them with the literal itself\n",
        "    pattern = re.compile(r\"<(STR|NUM|CHAR)_LIT:(.*?)>\", re.S)\n",
        "    lits = re.findall(pattern, code)\n",
        "    for lit in lits:\n",
        "        code = code.replace(f\"<{lit[0]}_LIT:{lit[1]}>\", lit[1])\n",
        "\n",
        "    # Find special tags and replace them with empty spaces\n",
        "    pattern = r'<([A-Z][^<>]*)>'\n",
        "    liners = re.findall(pattern, code)\n",
        "    for tag in liners:\n",
        "        code = code.replace(f'<{tag}>', ' ')\n",
        "\n",
        "    return code\n",
        "\n",
        "def read_jsonl_file(file_path):\n",
        "    \"\"\"\n",
        "    Reads a JSONL file and replaces special tags in the 'signature' and 'body' fields of each JSON object.\n",
        "\n",
        "    Parameters:\n",
        "        file_path (str): The path to the JSONL file.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries, each containing the modified JSON objects.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            json_obj = json.loads(line)\n",
        "            json_obj['signature'] = replace_tags(json_obj['signature'])\n",
        "            json_obj['body'] = replace_tags(json_obj['body'])\n",
        "            data.append(json_obj)\n",
        "    return data\n",
        "\n",
        "file_path = '/content/drive/MyDrive/CodeCompletion/CodeXGlue/test.jsonl'\n",
        "codexglue_test = read_jsonl_file(file_path)\n",
        "print(f'{codexglue_test[0]}\\n')\n",
        "\n",
        "columns_to_convert = ['is_single_expression', 'is_test', '0-20', '100+', '20-50', '50-100']\n",
        "\n",
        "file_path = '/content/drive/MyDrive/CodeCompletion/functions_df_inputs_outputs.csv'\n",
        "functions_df = pd.read_csv(file_path)\n",
        "functions_df[columns_to_convert] = functions_df[columns_to_convert].astype(str)\n",
        "print(f'{functions_df.iloc[0]}\\n')\n",
        "\n",
        "file_path = '/content/drive/MyDrive/CodeCompletion/context_functions_df.csv'\n",
        "context_functions_df = pd.read_csv(file_path)\n",
        "context_functions_df[columns_to_convert] = context_functions_df[columns_to_convert].astype(str)\n",
        "print(f'{context_functions_df.iloc[0]}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKCTUzirK_eS",
        "outputId": "d65fafb7-1d14-4e6c-f616-c2c1a113ba6f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'signature': 'def debug(user, message):', 'body': 'message_user(user, message, constants.DEBUG) ', 'docstring': 'Adds a message with the ``DEBUG`` level.\\n\\n:param user: User instance\\n:param message: Message to show', 'id': 'f4:m0'}\n",
            "\n",
            "Unnamed: 0                                                              0\n",
            "function_id                                                         27692\n",
            "signature               private fun bitIndex(elementIndex: Int, bitOff...\n",
            "body                    =\\n        elementIndex * ELEMENT_SIZE + bitOf...\n",
            "is_single_expression                                                 True\n",
            "is_test                                                             False\n",
            "0-20                                                                False\n",
            "100+                                                                False\n",
            "20-50                                                               False\n",
            "50-100                                                               True\n",
            "Name: 0, dtype: object\n",
            "\n",
            "signature                         fun test4(collection: Collection<A<*>>)\n",
            "body                    id(newA() in collection) id(newA<Int>() in col...\n",
            "docstring               Processes elements in the given collection by ...\n",
            "is_single_expression                                                False\n",
            "is_test                                                              True\n",
            "0-20                                                                False\n",
            "100+                                                                False\n",
            "20-50                                                                True\n",
            "50-100                                                              False\n",
            "Name: 0, dtype: object\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_function_python(code, signature):\n",
        "    \"\"\"\n",
        "    The fastest way to extract function from the output - regex.\n",
        "    But this approach has some downfalls, s.t. it does not consider nested function\n",
        "    \"\"\"\n",
        "    # Regular expression pattern to match function definitions\n",
        "    pattern = r\"def\\s+(\\w+)\\s*\\(.*?\\):[\\s\\S]*?(?=def|\\Z)\"\n",
        "\n",
        "    match = re.search(pattern, code)\n",
        "\n",
        "    if match and not match.group(0).split(signature, 1)[-1].strip() == '':\n",
        "        return match.group(0)\n",
        "    else:\n",
        "        return str(code)\n",
        "\n",
        "LANGUAGE_BUILDER_PATH = '/content/drive/MyDrive/CodeCompletion/parser/build/my-language.so'\n",
        "REPO_PATHS = '/content/drive/MyDrive/CodeCompletion/parser/tree-sitter-kotlin'\n",
        "\n",
        "tree_sitter.Language.build_library(LANGUAGE_BUILDER_PATH, [REPO_PATHS])\n",
        "KOTLIN_LANGUAGE = tree_sitter.Language(LANGUAGE_BUILDER_PATH, 'kotlin')\n",
        "parser_kotlin = tree_sitter.Parser()\n",
        "parser_kotlin.set_language(KOTLIN_LANGUAGE)\n",
        "\n",
        "def extract_function_kotlin(code, signature):\n",
        "  pattern = r\"fun\\s+(\\w+)\\s*\\(.*?\\)\\s*{[\\s\\S]*?(?=fun|\\Z)\"\n",
        "  match = re.search(pattern, code)\n",
        "\n",
        "  if match and not match.group(0).split(signature, 1)[-1].strip() == '':\n",
        "    return match.group(0)\n",
        "  else:\n",
        "    tree = parser_kotlin.parse(bytes(code, \"utf8\"))\n",
        "    root_node = tree.root_node\n",
        "    if root_node.children[0].type == 'function_declaration':\n",
        "      return root_node.children[0].text.decode('utf-8')\n",
        "    else:\n",
        "      return str(code)"
      ],
      "metadata": {
        "id": "0TRV1n5b0Kn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of usage\n",
        "code = \"\"\"\n",
        "def debug(user, message): print(f'{user} - {message}')\n",
        "def _check(path):  messageFiles = path.globChildren('*') for message in messageFiles:  if message.basename().endswith(''):  continue  receiver.message(message.getContent()) message.remove() return functools.partial(_check, path) def has_resuming(): return False\n",
        "\"\"\"\n",
        "\n",
        "# Extract the first function definition\n",
        "extracted_fun = extract_function_python(code, 'def debug(user, message):')\n",
        "print(extracted_fun)"
      ],
      "metadata": {
        "id": "PQk8_M-4-pTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation for evaluation"
      ],
      "metadata": {
        "id": "O6my74XDNpG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute average number of tokens\n",
        "Get mean values of number of tokens depending on length of signature and is_single_expression parameter.\n",
        "\n",
        "This code tokenizes text in the 'body' column, calculates the mean token count for each group defined by boolean columns, and aggregates the results.\n"
      ],
      "metadata": {
        "id": "gFryNctmUEiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "functions_df['tokenized_body'] = functions_df['body'].apply(lambda x: tokenizer.encode(x, add_special_tokens=False))\n",
        "\n",
        "# Convert bool columns to string for better groupby\n",
        "bool_cols = ['is_single_expression', 'is_test', '0-20', '100+', '20-50', '50-100']\n",
        "for col in bool_cols:\n",
        "    functions_df[col] = functions_df[col].astype(str)\n",
        "\n",
        "functions_df['num_tokens'] = functions_df['tokenized_body'].apply(len)\n",
        "mean_tokens_by_prompt = functions_df.groupby(['is_single_expression', 'is_test', '0-20', '100+', '20-50', '50-100'])['num_tokens'].mean().reset_index()\n",
        "mean_tokens_by_prompt['num_tokens'] = mean_tokens_by_prompt['num_tokens'].astype(int)\n",
        "\n",
        "print(mean_tokens_by_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMr2iurbSc7X",
        "outputId": "c05b2aa2-8e7e-4a4f-c049-5806741e3f01"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (12997 > 2048). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   is_single_expression is_test   0-20   100+  20-50 50-100  num_tokens\n",
            "0                 False   False  False  False  False   True          88\n",
            "1                 False   False  False  False   True  False          62\n",
            "2                 False   False  False   True  False  False         155\n",
            "3                 False   False   True  False  False  False          84\n",
            "4                 False    True  False  False  False   True         234\n",
            "5                 False    True  False  False   True  False         160\n",
            "6                 False    True  False   True  False  False         196\n",
            "7                 False    True   True  False  False  False         125\n",
            "8                  True   False  False  False  False   True          27\n",
            "9                  True   False  False  False   True  False          12\n",
            "10                 True   False  False   True  False  False          56\n",
            "11                 True   False   True  False  False  False          11\n",
            "12                 True    True  False  False  False   True         104\n",
            "13                 True    True  False  False   True  False          64\n",
            "14                 True    True  False   True  False  False          97\n",
            "15                 True    True   True  False  False  False          21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mean_tokens(row):\n",
        "  bool_values = row[bool_cols].values.flatten().tolist()\n",
        "  mean_token_count = mean_tokens_by_prompt[\n",
        "      (mean_tokens_by_prompt[bool_cols] == bool_values).all(axis=1)\n",
        "  ]['num_tokens'].iloc[0]\n",
        "  return mean_token_count\n",
        "\n",
        "print(\"Mean token count for the sample:\", get_mean_tokens(functions_df.iloc[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xtq3D_QvVPOl",
        "outputId": "1d4a3864-4796-4dc2-a2a4-81dda8f4b417"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean token count for the sample: 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions for constructing correct prompt"
      ],
      "metadata": {
        "id": "1CuhEGE52Glo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt_codex(dataset, index, num_examples, context=None, language='Python'):\n",
        "    \"\"\"\n",
        "    Creates a prompt for the CodeX model based on the dataset, index, and optional context.\n",
        "\n",
        "    Parameters:\n",
        "        dataset (list): A list of dictionaries representing code examples.\n",
        "        index (int): The index of the current code example.\n",
        "        num_examples (int): The number of examples to include in the prompt.\n",
        "        context (str, optional): Additional context to include in the prompt (default is None).\n",
        "        language (str, optional): The programming language of the code examples (default is 'Python').\n",
        "\n",
        "    Returns:\n",
        "        str: The generated prompt for the CodeX model.\n",
        "    \"\"\"\n",
        "    indices = random.sample(range(len(dataset)), num_examples)\n",
        "    prefix = f'\\nComplete code\\nLanguage: {language}\\n'\n",
        "    shots = '\\n'.join([f\"Example: {dataset[i]['signature']} {dataset[i]['body']}\" for i in indices])\n",
        "\n",
        "    data = dataset[index]\n",
        "    if context:\n",
        "        prompt = f\"{prefix}\\n{shots}\\n{context}\\nCode so far: {data['signature']}\"\n",
        "    else:\n",
        "        prompt = f\"{prefix}\\n{shots}\\nCode so far: {data['signature']}\"\n",
        "\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "Mec-v3tGN3Bo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt_kotlin(dataset, index, num_examples, context=None, language='Kotlin'):\n",
        "    \"\"\"\n",
        "    Creates a prompt for Kotlin code completion based on the dataset, index, and optional context.\n",
        "\n",
        "    Parameters:\n",
        "        dataset (DataFrame): A DataFrame containing Kotlin code examples.\n",
        "        index (int): The index of the current code example.\n",
        "        num_examples (int): The number of examples to include in the prompt.\n",
        "        context (str, optional): Additional context to include in the prompt (default is None).\n",
        "        language (str, optional): The programming language of the code examples (default is 'Kotlin').\n",
        "\n",
        "    Returns:\n",
        "        str: The generated prompt for Kotlin code completion.\n",
        "    \"\"\"\n",
        "    indices = random.sample(range(len(dataset)), num_examples)\n",
        "    prefix = f'Complete code\\nLanguage: {language}\\n'\n",
        "    shots = '\\n'.join([f\"Example: {dataset.iloc[i]['signature']} {dataset.iloc[i]['body']}\" for i in indices])\n",
        "\n",
        "    data = dataset.iloc[index]\n",
        "    if context:\n",
        "        prompt = f\"{prefix}\\n{shots}\\n{context}\\nCode so far: {data['signature']}\"\n",
        "    else:\n",
        "        prompt = f\"{prefix}\\n{shots}\\nCode so far: {data['signature']}\"\n",
        "\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "sUAvFm4KqpgQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function for metrics computing **(edit_sim, CHRF Score, ROUGE-1 Score)**"
      ],
      "metadata": {
        "id": "MK3jyTRbdTye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We don't need stemmer here, because we want to compare full tokens\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=False, tokenizer=tokenizer)\n",
        "\n",
        "def compute_metrics(ref, text):\n",
        "    \"\"\"\n",
        "    Computes various evaluation metrics between a reference and generated text.\n",
        "\n",
        "    Parameters:\n",
        "        ref (str): The reference text.\n",
        "        text (str): The generated text.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the computed metrics (edit_sim, chrf_score, rogue1_precision, rogue1_recall, rogue1_fmeasure).\n",
        "    \"\"\"\n",
        "    edit_sim = fuzz.ratio(text, ref)\n",
        "    chrf_score = nltk.translate.chrf_score.chrf_precision_recall_fscore_support(ref, text, 4)[2]  # Return only f-score\n",
        "    rogue_scores = scorer.score(ref, text)['rouge1']\n",
        "    return edit_sim, chrf_score, rogue_scores.precision, rogue_scores.recall, rogue_scores.fmeasure\n",
        "\n",
        "def print_metrics(metrics):\n",
        "  metrics_names = ['edit_sim', 'chrf_score', 'precision', 'recall', 'f-measure']\n",
        "  print(\"Results: \")\n",
        "  for i in range(len(metrics)):\n",
        "    if i == 2:\n",
        "      token = scorer._tokenizer\n",
        "      print(f\"Generated tokens: \\n\\tGenerated code tokenized: {token(generated_code)['input_ids'][:25]}... \\n\\tReference code tokenized: {token(reference_code)['input_ids'][:25]}...\")\n",
        "    print(f\"{metrics_names[i]}: {metrics[i]}\")"
      ],
      "metadata": {
        "id": "TtvrA6MQdJcu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of computing metrics\n",
        "generated_code = '''\n",
        "def loads(s, strip_comments=False, **kw):\n",
        "    return [Node.create(s, **kw) for s in s.split('\\n') if s]\n",
        "'''\n",
        "\n",
        "reference_code = '''\n",
        "def loads(s, strip_comments=False, **kw):\n",
        "    kw[''] = strip_comments\n",
        "    return [parse_node(ss.strip(), **kw) for ss in s.split(';') if ss.strip()]\n",
        "'''\n",
        "\n",
        "metrics = compute_metrics(reference_code, generated_code)\n",
        "print_metrics(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eC9Ggw740q6c",
        "outputId": "54abc13c-ce7f-462a-cf0c-961bf4a3e5ea"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results: \n",
            "edit_sim: 76\n",
            "chrf_score: 0.5407303370786517\n",
            "Generated tokens: \n",
            "\tGenerated code tokenized: [198, 4299, 15989, 7, 82, 11, 10283, 62, 15944, 28, 25101, 11, 12429, 46265, 2599, 198, 50284, 7783, 685, 19667, 13, 17953, 7, 82, 11]... \n",
            "\tReference code tokenized: [198, 4299, 15989, 7, 82, 11, 10283, 62, 15944, 28, 25101, 11, 12429, 46265, 2599, 198, 50284, 46265, 58, 7061, 60, 796, 10283, 62, 15944]...\n",
            "precision: 0.8536585365853658\n",
            "recall: 0.625\n",
            "f-measure: 0.7216494845360826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation (code without description)"
      ],
      "metadata": {
        "id": "gE0-SHIT1vUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kotlin_data = functions_df[0:10000]\n",
        "\n",
        "kotlin_data_context = context_functions_df\n",
        "\n",
        "python_data = codexglue_test[0:10000]"
      ],
      "metadata": {
        "id": "CPN3tcEJNgn9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate code completion for Python (10_000 of samples)\n",
        "Code calculates the mean values of code generation metrics for two sets of results: one without context and another with context. Prints the mean values and exports them to CSV files for further analysis."
      ],
      "metadata": {
        "id": "frsiOhphUiw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation_pipeline_python(args, dataset, descr=False):\n",
        "    \"\"\"\n",
        "    Evaluates Python code generation using a pre-trained model.\n",
        "\n",
        "    Parameters:\n",
        "        args (dict): Model configuration parameters.\n",
        "        dataset (list): List of dictionaries containing code examples.\n",
        "        descr (bool, optional): Whether to include function descriptions as context (default is False).\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: DataFrame containing evaluation results for each code example.\n",
        "    \"\"\"\n",
        "    args_copy = args.copy()\n",
        "    result_df = pd.DataFrame(dataset)\n",
        "    new_columns = ['edit_sim', 'chrf_score', 'precision', 'recall', 'f-measure']\n",
        "    result_df[new_columns] = 0.0\n",
        "\n",
        "    for i in tqdm(range(len(dataset))):\n",
        "        if descr:\n",
        "            prompt = create_prompt_codex(dataset, i, 2, dataset[i]['docstring'])\n",
        "        else:\n",
        "            prompt = create_prompt_codex(dataset, i, 2)\n",
        "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "        # Update max_length based on input prompt length\n",
        "        args_copy['max_length'] = len(inputs[0]) + args['max_length']\n",
        "\n",
        "        outputs = model.generate(inputs, **args_copy)\n",
        "\n",
        "        text = tokenizer.batch_decode(outputs)[0]\n",
        "        function = text.split(\"Code so far: \", 1)[-1]\n",
        "        function = extract_function_python(function, dataset[i]['signature'])\n",
        "        function_body = function.split(dataset[i]['signature'], 1)[-1]\n",
        "        metrics = compute_metrics(dataset[i]['body'], function_body)\n",
        "\n",
        "        result_df.loc[i, new_columns] = metrics\n",
        "\n",
        "    return result_df\n",
        "\n",
        "config = {'max_length': 75, 'do_sample': True, 'temperature': 0.7, 'top_k': 5, 'top_p': 0.8, 'num_return_sequences': 1}\n",
        "result_no_context = evaluation_pipeline_python(config, python_data)\n",
        "result_context = evaluation_pipeline_python(config, python_data, descr=True)"
      ],
      "metadata": {
        "id": "Sv5eSTxmQPGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_values_no_context = result_no_context[['edit_sim', 'chrf_score', 'precision', 'recall', 'f-measure']].mean(axis=0)\n",
        "print(mean_values_no_context)\n",
        "mean_values_no_context.to_csv('result_no_context.csv')\n",
        "\n",
        "mean_values_context = result_context[['edit_sim', 'chrf_score', 'precision', 'recall', 'f-measure']].mean(axis=0)\n",
        "print(mean_values_context)\n",
        "mean_values_context.to_csv('result_context.csv')"
      ],
      "metadata": {
        "id": "YYEHw5D1YWMQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ab8c3b2-e3e5-4822-8637-4d329ffb74dc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "edit_sim      31.950000\n",
            "chrf_score     0.168040\n",
            "precision      0.287897\n",
            "recall         0.303383\n",
            "f-measure      0.224306\n",
            "dtype: float64\n",
            "edit_sim      29.230000\n",
            "chrf_score     0.140260\n",
            "precision      0.214698\n",
            "recall         0.273709\n",
            "f-measure      0.182082\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation_pipeline_kotlin(args, dataset, descr=False):\n",
        "    \"\"\"\n",
        "    Evaluates Kotlin code generation using a pre-trained model.\n",
        "\n",
        "    Parameters:\n",
        "        args (dict): Model configuration parameters.\n",
        "        dataset (DataFrame): DataFrame containing Kotlin code examples.\n",
        "        descr (bool, optional): Whether to include function descriptions as context (default is False).\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: DataFrame containing evaluation results for each code example.\n",
        "    \"\"\"\n",
        "    args_copy = args.copy()\n",
        "    result_df = dataset.copy()\n",
        "    new_columns = ['edit_sim', 'chrf_score', 'precision', 'recall', 'f-measure']\n",
        "    result_df[new_columns] = 0.0\n",
        "\n",
        "    for i in tqdm(range(len(dataset))):\n",
        "        if descr:\n",
        "            prompt = create_prompt_kotlin(dataset, i, 2, dataset.iloc[i]['docstring'])\n",
        "        else:\n",
        "            prompt = create_prompt_kotlin(dataset, i, 2)\n",
        "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "        # Using mean_tokens_by_prompt generated above, we got much better results (+8% for each metric)\n",
        "        length = get_mean_tokens(dataset.iloc[i])\n",
        "        args_copy['max_length'] = len(inputs[0]) + length + 50\n",
        "        outputs = model.generate(inputs, **args_copy)\n",
        "\n",
        "        text = tokenizer.batch_decode(outputs)[0]\n",
        "        function = text.split(\"Code so far: \", 1)[-1]\n",
        "        function = extract_function_kotlin(function, dataset.iloc[i]['signature'])\n",
        "        function_body = function.split(dataset.iloc[i]['signature'], 1)[-1]\n",
        "        metrics = compute_metrics(dataset.iloc[i]['body'], function_body)\n",
        "\n",
        "        result_df.loc[i, new_columns] = metrics\n",
        "\n",
        "    return result_df"
      ],
      "metadata": {
        "id": "a-sDcV0sqZev"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Search for Code Generation\n",
        "This script conducts a hyperparameter search to optimize the performance of a code generation model. It iterates over different combinations of parameters for both beam search and sampling methods, evaluates each combination, and identifies the configuration with the highest edit similarity score."
      ],
      "metadata": {
        "id": "wyEy_dzRUHmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "existing_config = {'max_length': 0}\n",
        "\n",
        "possible_beam_params = {'num_beams': [2, 3, 4, 5]}\n",
        "possible_beam_search_params = {'num_return_sequences': [1, 2, 3], 'length_penalty': [0.8, 0.9, 1.0]}\n",
        "\n",
        "possible_sampling_params = {'do_sample': [True], 'temperature': [0.7, 0.8, 0.9]}\n",
        "possible_sampling_search_params = {'top_k': [5, 10, 20], 'top_p': [0.7, 0.8, 0.9], 'num_return_sequences': [1]}\n",
        "\n",
        "best_score = float('-inf')\n",
        "best_config = {}\n",
        "\n",
        "# Search for best parameters for beam search\n",
        "for beam_params in itertools.product(*possible_beam_params.values()):\n",
        "    for beam_search_params in itertools.product(*possible_beam_search_params.values()):\n",
        "        config = existing_config.copy()\n",
        "        config.update(dict(zip(possible_beam_params.keys(), beam_params)))\n",
        "        config.update(dict(zip(possible_beam_search_params.keys(), beam_search_params)))\n",
        "        print(config)\n",
        "\n",
        "        result = evaluation_pipeline_kotlin(config, functions_df[0:100])\n",
        "        mean_values_no_context = result[['edit_sim', 'chrf_score', 'precision', 'recall', 'f-measure']].mean(axis=0)\n",
        "        print(f\"\\nEdit sim: {mean_values_no_context['edit_sim']}\")\n",
        "        if mean_values_no_context['edit_sim'] > best_score:\n",
        "            best_score = mean_values_no_context['edit_sim']\n",
        "            best_config = config\n",
        "            print(f\"New best value: {mean_values_no_context['edit_sim']} with config {best_config}\")\n",
        "\n",
        "# Search for best parameters for sampling\n",
        "for sampling_params in itertools.product(*possible_sampling_params.values()):\n",
        "    for sampling_search_params in itertools.product(*possible_sampling_search_params.values()):\n",
        "        config = existing_config.copy()\n",
        "        config.update(dict(zip(possible_sampling_params.keys(), sampling_params)))\n",
        "        config.update(dict(zip(possible_sampling_search_params.keys(), sampling_search_params)))\n",
        "        print(config)\n",
        "\n",
        "        result = evaluation_pipeline_kotlin(config, functions_df[0:100])\n",
        "        mean_values_no_context = result[['edit_sim', 'chrf_score', 'precision', 'recall', 'f-measure']].mean(axis=0)\n",
        "        print(f\"\\nEdit sim: {mean_values_no_context['edit_sim']}\")\n",
        "        if mean_values_no_context['edit_sim'] > best_score:\n",
        "            best_score = mean_values_no_context['edit_sim']\n",
        "            best_config = config\n",
        "            print(f\"New best value: {mean_values_no_context['edit_sim']} with config {best_config}\")\n",
        "\n",
        "# Best config is {'max_length': 0, 'do_sample': True, 'temperature': 0.7, 'top_k': 5, 'top_p': 0.8, 'num_return_sequences': 1}"
      ],
      "metadata": {
        "id": "8OpbSIBjTcZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate every type of Kotlin function (4_000 = 250 * 16 samples)\n",
        "This script analyzes code generation metrics across different function types. It computes the mean values of these metrics for each function type, based on specified criteria, and stores the results in a DataFrame for further analysis."
      ],
      "metadata": {
        "id": "lHUQkKEZUL9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {'max_length': 0, 'do_sample': True, 'temperature': 0.7, 'top_k': 5, 'top_p': 0.8, 'num_return_sequences': 1}\n",
        "\n",
        "fun_type_col = ['is_single_expression', 'is_test', '0-20', '100+', '20-50', '50-100']\n",
        "metrics = ['edit_sim', 'chrf_score', 'precision', 'recall', 'f-measure']\n",
        "\n",
        "results_by_type = pd.DataFrame(columns=fun_type_col + metrics)\n",
        "grouped = mean_tokens_by_prompt.groupby(fun_type_col)\n",
        "\n",
        "for name, group in grouped:\n",
        "  df = functions_df[(functions_df[fun_type_col] == name).all(axis=1)].head(250)\n",
        "  df = df.reset_index(drop=True)\n",
        "\n",
        "  result = evaluation_pipeline_kotlin(config, df)\n",
        "  mean_values = result[metrics].mean(axis=0).tolist()\n",
        "\n",
        "  results_by_type.loc[len(results_by_type)] = list(name) + mean_values"
      ],
      "metadata": {
        "id": "duPIM1RGtafZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results_by_type)\n",
        "results_by_type.to_csv('results_by_type1.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPaBA-RM6jsr",
        "outputId": "d3446b26-22b2-41a0-e426-0696616ab525"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   is_single_expression is_test   0-20   100+  20-50 50-100  edit_sim  \\\n",
            "0                 False   False  False  False  False   True     38.08   \n",
            "1                 False   False  False  False   True  False     29.60   \n",
            "2                 False   False  False   True  False  False     36.92   \n",
            "3                 False   False   True  False  False  False     32.32   \n",
            "4                 False    True  False  False  False   True     31.88   \n",
            "5                 False    True  False  False   True  False     31.64   \n",
            "6                 False    True  False   True  False  False     29.96   \n",
            "7                 False    True   True  False  False  False     21.72   \n",
            "8                  True   False  False  False  False   True     35.88   \n",
            "9                  True   False  False  False   True  False     26.44   \n",
            "10                 True   False  False   True  False  False     33.16   \n",
            "11                 True   False   True  False  False  False     27.24   \n",
            "12                 True    True  False  False  False   True     35.04   \n",
            "13                 True    True  False  False   True  False     27.28   \n",
            "14                 True    True  False   True  False  False     22.60   \n",
            "15                 True    True   True  False  False  False     26.00   \n",
            "\n",
            "    chrf_score  precision    recall  f-measure  \n",
            "0     0.314951   0.367657  0.438023   0.325907  \n",
            "1     0.197481   0.324354  0.316135   0.231195  \n",
            "2     0.278596   0.440599  0.386026   0.323652  \n",
            "3     0.191814   0.343816  0.323252   0.248844  \n",
            "4     0.226207   0.311770  0.322475   0.250783  \n",
            "5     0.183468   0.332375  0.299894   0.233466  \n",
            "6     0.185215   0.313348  0.249019   0.210197  \n",
            "7     0.089830   0.194551  0.214834   0.142087  \n",
            "8     0.174444   0.233465  0.218356   0.198273  \n",
            "9     0.083114   0.100648  0.181464   0.107197  \n",
            "10    0.236558   0.288601  0.332410   0.234764  \n",
            "11    0.055275   0.102825  0.120739   0.087793  \n",
            "12    0.257268   0.337614  0.343543   0.256637  \n",
            "13    0.077798   0.163829  0.197966   0.142362  \n",
            "14    0.104866   0.168541  0.156354   0.122062  \n",
            "15    0.019492   0.038923  0.097559   0.051449  \n"
          ]
        }
      ]
    }
  ]
}