{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model Fine-tuning"
      ],
      "metadata": {
        "id": "F89F4UeLYt1i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiwGuFg_YbuJ",
        "outputId": "af8d0718-ff1b-4f15-8dde-81dfb4e7bb19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy\n",
        "!pip install python-Levenshtein\n",
        "!pip install tree-sitter\n",
        "!pip install rouge-score\n",
        "!pip install accelerate transformers einops datasets peft bitsandbytes --upgrade"
      ],
      "metadata": {
        "id": "9TOmAYwZYgaG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
        "import datetime\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from fuzzywuzzy import fuzz\n",
        "import Levenshtein\n",
        "import nltk\n",
        "from rouge_score import rouge_scorer\n",
        "import tree_sitter\n",
        "from peft import PeftModel, LoraConfig, get_peft_model\n",
        "from datasets import load_dataset, Dataset\n",
        "import os"
      ],
      "metadata": {
        "id": "nzWDwLJZYiFh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.set_default_device(\"cuda\")\n",
        "\n",
        "print(\"Loading model...\")\n",
        "time = datetime.datetime.now()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "'''\n",
        "Chosen parameters:\n",
        "- bnb_4bit_use_double_quant: enables a second quantization after the first one to save an additional 0.4 bits per parameter\n",
        "- bnb_4bit_quant_type: non-float 4-bit\n",
        "- bnb_4bit_compute_dtype: we need to specify a computation type because while nf4 stores weights in 4-bit type, the computation still happens in 16/32 bits\n",
        "'''\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/phi-1_5\",\n",
        "    device_map={\"\":0},  # device index of 0 refers to the first available GPU device\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "time1 = datetime.datetime.now()\n",
        "print(f\"Model loaded. Time to load the model: {time1 - time}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdTyhIxgYmzi",
        "outputId": "dd6b3d78-52ce-4f1d-f0cc-1275e7270acf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...\n",
            "Model loaded. Time to load the model: 0:00:02.430820\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6rIqSPoEMd8",
        "outputId": "fd154853-359c-427a-f48d-73974bab081c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PhiForCausalLM(\n",
              "  (model): PhiModel(\n",
              "    (embed_tokens): Embedding(51200, 2048)\n",
              "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-23): 24 x PhiDecoderLayer(\n",
              "        (self_attn): PhiSdpaAttention(\n",
              "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "          (k_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "          (v_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "          (dense): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "          (rotary_emb): PhiRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): PhiMLP(\n",
              "          (activation_fn): NewGELUActivation()\n",
              "          (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "        )\n",
              "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (final_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=51200, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Chosen parameters:\n",
        "- r: The number of buckets used in the LORA algorithm for quantization.\n",
        "- target_modules:\n",
        "  - 'dense': A fully connected layer in a neural network (lated in PhiSdpaAttention).\n",
        "  - 'fc2': The second fully connected layer in Phi - Multi Layered Perceptron.\n",
        "  - 'q_proj', 'k_proj', 'v_proj': Projection layers used in the attention mechanism of transformer models.\n",
        "    They project input embeddings into query, key, and value vectors for attention computation. (Located in PhiSdpaAttention)\n",
        "'''\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"dense\", \"fc2\", \"q_proj\", \"k_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QTmbwnpID02",
        "outputId": "73dc3f31-e5fa-4f66-d2fd-0f1f6ef255a1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 10,223,616 || all params: 1,428,494,336 || trainable%: 0.7156917421617274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_tags(code):\n",
        "    \"\"\"\n",
        "    Replaces special tags in the input code with their corresponding literals or empty strings.\n",
        "    Original function is here:\n",
        "    https://github.com/microsoft/CodeXGLUE/blob/main/Code-Code/CodeCompletion-line/evaluator/evaluator.py\n",
        "\n",
        "    Parameters:\n",
        "        code (str): The input code containing special tags.\n",
        "\n",
        "    Returns:\n",
        "        str: The code with tags replaced by literals or empty strings.\n",
        "    \"\"\"\n",
        "    # Replace special tags with their corresponding literals or empty strings\n",
        "    code = code.replace(\"<NUM_LIT>\", \"0\").replace(\"<STR_LIT>\", \"\").replace(\"<CHAR_LIT>\", \"\")\n",
        "\n",
        "    # Find literals enclosed in special tags and replace them with the literal itself\n",
        "    pattern = re.compile(r\"<(STR|NUM|CHAR)_LIT:(.*?)>\", re.S)\n",
        "    lits = re.findall(pattern, code)\n",
        "    for lit in lits:\n",
        "        code = code.replace(f\"<{lit[0]}_LIT:{lit[1]}>\", lit[1])\n",
        "\n",
        "    # Find special tags and replace them with empty spaces\n",
        "    pattern = r'<([A-Z][^<>]*)>'\n",
        "    liners = re.findall(pattern, code)\n",
        "    for tag in liners:\n",
        "        code = code.replace(f'<{tag}>', ' ')\n",
        "\n",
        "    return code\n",
        "\n",
        "def read_jsonl_file(file_path):\n",
        "    \"\"\"\n",
        "    Reads a JSONL file and replaces special tags in the 'signature' and 'body' fields of each JSON object.\n",
        "\n",
        "    Parameters:\n",
        "        file_path (str): The path to the JSONL file.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries, each containing the modified JSON objects.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            json_obj = json.loads(line)\n",
        "            json_obj['signature'] = replace_tags(json_obj['signature'])\n",
        "            json_obj['body'] = replace_tags(json_obj['body'])\n",
        "            data.append(json_obj)\n",
        "    return data\n",
        "\n",
        "file_path = '/content/drive/MyDrive/CodeCompletion/CodeXGlue/test.jsonl'\n",
        "codexglue_test = read_jsonl_file(file_path)\n",
        "print(f'{codexglue_test[0]}\\n')\n",
        "\n",
        "columns_to_convert = ['is_single_expression', 'is_test', '0-20', '100+', '20-50', '50-100']\n",
        "\n",
        "file_path = '/content/drive/MyDrive/CodeCompletion/functions_df_inputs_outputs.csv'\n",
        "functions_df = pd.read_csv(file_path)\n",
        "functions_df[columns_to_convert] = functions_df[columns_to_convert].astype(str)\n",
        "print(f'{functions_df.iloc[0]}\\n')\n",
        "\n",
        "file_path = '/content/drive/MyDrive/CodeCompletion/context_functions_df.csv'\n",
        "context_functions_df = pd.read_csv(file_path)\n",
        "context_functions_df[columns_to_convert] = context_functions_df[columns_to_convert].astype(str)\n",
        "print(f'{context_functions_df.iloc[0]}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Mb1EOOvYo8O",
        "outputId": "02d72f63-7a58-4946-bad0-0e136de68673"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'signature': 'def debug(user, message):', 'body': 'message_user(user, message, constants.DEBUG) ', 'docstring': 'Adds a message with the ``DEBUG`` level.\\n\\n:param user: User instance\\n:param message: Message to show', 'id': 'f4:m0'}\n",
            "\n",
            "Unnamed: 0                                                              0\n",
            "function_id                                                         27692\n",
            "signature               private fun bitIndex(elementIndex: Int, bitOff...\n",
            "body                    =\\n        elementIndex * ELEMENT_SIZE + bitOf...\n",
            "is_single_expression                                                 True\n",
            "is_test                                                             False\n",
            "0-20                                                                False\n",
            "100+                                                                False\n",
            "20-50                                                               False\n",
            "50-100                                                               True\n",
            "Name: 0, dtype: object\n",
            "\n",
            "signature                         fun test4(collection: Collection<A<*>>)\n",
            "body                    id(newA() in collection) id(newA<Int>() in col...\n",
            "docstring               Processes elements in the given collection by ...\n",
            "is_single_expression                                                False\n",
            "is_test                                                              True\n",
            "0-20                                                                False\n",
            "100+                                                                False\n",
            "20-50                                                                True\n",
            "50-100                                                              False\n",
            "Name: 0, dtype: object\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sample):\n",
        "    tokenized_text = tokenizer(sample[\"text\"], padding=True, truncation=True, max_length=256)\n",
        "    return tokenized_text\n",
        "\n",
        "functions_df[\"text\"] = functions_df[[\"signature\", \"body\"]].apply(lambda x: \"Prompt: \" + x[\"signature\"] + \" Completion: \" + x[\"body\"], axis=1)\n",
        "print(functions_df.iloc[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOauZDM2dFei",
        "outputId": "0c569fa1-d55e-4550-87d6-a0f0b8dd7cad"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unnamed: 0                                                              0\n",
            "function_id                                                         27692\n",
            "signature               private fun bitIndex(elementIndex: Int, bitOff...\n",
            "body                    =\\n        elementIndex * ELEMENT_SIZE + bitOf...\n",
            "is_single_expression                                                 True\n",
            "is_test                                                             False\n",
            "0-20                                                                False\n",
            "100+                                                                False\n",
            "20-50                                                               False\n",
            "50-100                                                               True\n",
            "text                    Prompt: private fun bitIndex(elementIndex: Int...\n",
            "Name: 0, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = Dataset.from_pandas(functions_df)\n",
        "tokenized_data = data.map(tokenize, batched=True, desc=\"Tokenizing data\", remove_columns=data.column_names)\n",
        "\n",
        "tokenized_data[0]"
      ],
      "metadata": {
        "id": "hU3O-cYZni1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"phi-1_5-finetuned-kotlin\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=100,\n",
        "    max_steps=1000,\n",
        "    num_train_epochs=1\n",
        ")"
      ],
      "metadata": {
        "id": "hva2Q4LYiytn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_data,\n",
        "    args=training_arguments,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "LaDPMqUDhnF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"phi-1_5-finetuned-kotlin\")"
      ],
      "metadata": {
        "id": "zDU636DKpFuE"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=torch.float32)\n",
        "\n",
        "peft_model = PeftModel.from_pretrained(model, \"phi-1_5-finetuned-kotlin\", from_transformers=True)\n",
        "\n",
        "model = peft_model.merge_and_unload()\n",
        "\n",
        "# Now we can load fine-tuned model in evaluate_pretrained.ipynb and try to evaluate them"
      ],
      "metadata": {
        "id": "KJerQjBCpHQO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}