# Method name generation

This assignment focuses on working on a code completion task. 
First, choose a large open-source project mostly written in Kotlin, 
extract all its Kotlin code to create a dataset. 
Then, adapt a pre-trained Python-focused Transformer model like Phi-1.5 for code completion, 
and test its performance on both Kotlin and Python datasets. 
Finally, fine-tune the model using the Kotlin dataset and compare its quality 
changes on both Kotlin and Python data.

## Steps of solution

1. **Extracting data from the [Kotlin Project](https://github.com/JetBrains/kotlin)**:
* Getting all Kotlin files from the project.
* Extracting methods from obtained Kotlin files using the [tree-sitter](https://github.com/fwcd/tree-sitter-kotlin) library for kotlin.
* Saving extracted methods to a DataFrame with the following features: `function_id, simple_identifier, function_value_parameters, user_type, modifiers, function_body, type_parameters, is_single_expression, is_test`.

2. **Preprocessing data for evaluation and fine-tuning**:

* According to my assumption, the model would perform better in code completion for signatures with greater length.
  That's why I created flags for different length of the signature string in dataset.
* The version of DataFrame after previous operation has the following
  features: `function_id, signature, body, is_single_expression, is_test, 0-20, 20-50, 50-100, 100+`. And it looks like this:

| function_id | signature                         | body                                               | is_single_expression | is_test | 0-20  | 20-50 | 50-100 | 100+  |
|------------:|:----------------------------------|:---------------------------------------------------|:---------------------|:--------|:------|:------|:-------|:------|
|       83134 | override fun toString(): String + | = \n file class ${jClass.classId.asSingleFqName()} | True                 | False   | False | True  | False  | False |

* Also, I used ChatGPT to create a small dataset of 200 functions for which a code summary is generated.
  It is a way to simulate human behavior, i.e., writing a core idea of the function before its declaration. It may be used in further work on project.

3. **Evaluating predictions using a pre-trained model**:
* Using a pre-trained model to predict method's body for each method signature.
* Metrics:
  - **Edit Similarity (fuzz.ratio):** Measures overall string similarity, suitable for code completion due to its ability to capture similarity regardless of word order or tokenization.

  - **CHRF Score (Character n-gram F-score, n=4):** Evaluates similarity at the character level, capturing fine-grained details and being robust to variations in word boundaries and morphology.

  - **ROUGE-1 Score (Recall-Oriented Understudy for Gisting Evaluation):** Assesses the presence of individual tokens (generated by either build-in tokenizer or a given one), providing insights into the adequacy of the generated code for code completion tasks.
  In results this score is shown as **precision, recall and f-measure** for single tokens. We use this metric instead of BLEU, because it is better to compare tokens than words in this task.
    - **Precision** (also called positive predictive value) - is the fraction of relevant instances among the retrieved instances. 
    - **Recall** (also known as sensitivity) - is the fraction of relevant instances that were retrieved. 
    - **F-1 measure** - their harmonic mean.


* Evaluation results (**Python, pre-trained model**):

| Metric           | Without Context   | With Context      |
|:-----------------|------------------:|------------------:|
| edit_sim         |              29.8 |             30.59 |
| chrf_score       |            0.1428 |            0.1421 |
| precision        |            0.2398 |            0.2766 |
| recall           |            0.2631 |            0.2595 |
| f-measure        |            0.1918 |            0.1999 |

* **Analysis**:
  - `edit_sim`: Results with context slightly outperform those without, suggesting context improves similarity.
  - `chrf_score`: Minimal difference observed between the two conditions.
  - `precision`: Higher with context, indicating fewer false positives.
  - `recall`: Slightly higher without context, suggesting it captures more relevant information.
  - `f-measure`: Shows a slight improvement with context, indicating a better balance between precision and recall.


* Evaluation results (**Kotlin, pre-trained model**):

| is_single_expression   | is_test   | 0-20   | 20-50   | 50-100   | 100+   |   edit_sim |   chrf_score |   precision |   recall |   f-measure |
|:-----------------------|:----------|:-------|:--------|:---------|:-------|-----------:|-------------:|------------:|---------:|------------:|
| False                  | False     | False  | False   | True     | False  |      38.08 |        0.315 |       0.368 |    0.438 |       0.326 |
| False                  | False     | False  | True    | False    | False  |      29.6  |        0.197 |       0.324 |    0.316 |       0.231 |
| False                  | False     | False  | False   | False    | True   |      36.92 |        0.279 |       0.441 |    0.386 |       0.324 |
| False                  | False     | True   | False   | False    | False  |      32.32 |        0.192 |       0.344 |    0.323 |       0.249 |
| False                  | True      | False  | False   | True     | False  |      31.88 |        0.226 |       0.312 |    0.322 |       0.251 |
| False                  | True      | False  | True    | False    | False  |      31.64 |        0.183 |       0.332 |    0.3   |       0.233 |
| False                  | True      | False  | False   | False    | True   |      29.96 |        0.185 |       0.313 |    0.249 |       0.21  |
| False                  | True      | True   | False   | False    | False  |      21.72 |        0.09  |       0.195 |    0.215 |       0.142 |
| True                   | False     | False  | False   | True     | False  |      35.88 |        0.174 |       0.233 |    0.218 |       0.198 |
| True                   | False     | False  | True    | False    | False  |      26.44 |        0.083 |       0.101 |    0.181 |       0.107 |
| True                   | False     | False  | False   | False    | True   |      33.16 |        0.237 |       0.289 |    0.332 |       0.235 |
| True                   | False     | True   | False   | False    | False  |      27.24 |        0.055 |       0.103 |    0.121 |       0.088 |
| True                   | True      | False  | False   | True     | False  |      35.04 |        0.257 |       0.338 |    0.344 |       0.257 |
| True                   | True      | False  | True    | False    | False  |      27.28 |        0.078 |       0.164 |    0.198 |       0.142 |
| True                   | True      | False  | False   | False    | True   |      22.6  |        0.105 |       0.169 |    0.156 |       0.122 |
| True                   | True      | True   | False   | False    | False  |      26    |        0.019 |       0.039 |    0.098 |       0.051 |

* **Analysis**:

  - `is_single_expression`: Rows with `False` values tend to have higher scores, indicating easier or more accurate generation of non-single expressions for pre-trained model.
  - `is_test`: Rows with `True` values generally have lower `chrf_score` and `edit_sim` scores, suggesting challenges in generating accurate test cases.
  - Function signature lengths (`0-20`, `20-50`, `50-100`, `100+`) influence evaluation metrics, with some length ranges showing higher precision, recall, and F-measure scores. Best results were showed by functions with length of signature between 50 and 100.
