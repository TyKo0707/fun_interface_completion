{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate pre-trained model"
      ],
      "metadata": {
        "id": "hIBz6WkK83-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOykphLYjVsi",
        "outputId": "7e33fd60-8d75-41e9-c5d2-319f886d61b9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy\n",
        "!pip install python-Levenshtein"
      ],
      "metadata": {
        "id": "pjxDCjQDVH7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Open model & load data"
      ],
      "metadata": {
        "id": "1CijgZdh80-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import datetime\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "from fuzzywuzzy import fuzz\n",
        "import Levenshtein\n",
        "# Info is here: https://github.com/seatgeek/thefuzz"
      ],
      "metadata": {
        "id": "HUVRyeWvVClJ"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrzVyonejB05"
      },
      "outputs": [],
      "source": [
        "torch.set_default_device(\"cuda\")\n",
        "\n",
        "print(\"Loading model...\")\n",
        "time = datetime.datetime.now()\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", torch_dtype=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n",
        "time1 = datetime.datetime.now()\n",
        "print(f\"Model loaded. Time to load the model: {time1 - time}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_tags(code):\n",
        "    # Original function is here:\n",
        "    # https://github.com/microsoft/CodeXGLUE/blob/main/Code-Code/CodeCompletion-line/evaluator/evaluator.py\n",
        "    code = code.replace(\"<NUM_LIT>\", \"0\").replace(\"<STR_LIT>\", \"\").replace(\"<CHAR_LIT>\", \"\")\n",
        "    pattern = re.compile(r\"<(STR|NUM|CHAR)_LIT:(.*?)>\", re.S)\n",
        "    lits = re.findall(pattern, code)\n",
        "    for lit in lits:\n",
        "        code = code.replace(f\"<{lit[0]}_LIT:{lit[1]}>\", lit[1])\n",
        "\n",
        "    pattern = r'<([A-Z][^<>]*)>'\n",
        "    liners = re.findall(pattern, code)\n",
        "    for tag in liners:\n",
        "        code = code.replace(f'<{tag}>', ' ')\n",
        "    return code\n",
        "\n",
        "def read_jsonl_file(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            json_obj = json.loads(line)\n",
        "            json_obj['signature'] = replace_tags(json_obj['signature'])\n",
        "            json_obj['body'] = replace_tags(json_obj['body'])\n",
        "            data.append(json_obj)\n",
        "    return data\n",
        "\n",
        "file_path = '/content/drive/MyDrive/CodeCompletion/CodeXGlue/test.jsonl'\n",
        "codexglue_test = read_jsonl_file(file_path)\n",
        "print(codexglue_test[0])\n",
        "\n",
        "file_path = '/content/drive/MyDrive/CodeCompletion/functions_df_inputs_outputs.csv'\n",
        "functions_df = pd.read_csv(file_path)\n",
        "print(functions_df.iloc[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3zscWI_9yfy",
        "outputId": "8922fc79-f6e8-4d2e-e49d-7621a31f661a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'signature': 'def debug(user, message):', 'body': 'message_user(user, message, constants.DEBUG) ', 'docstring': 'Adds a message with the ``DEBUG`` level.\\n\\n:param user: User instance\\n:param message: Message to show', 'id': 'f4:m0'}\n",
            "Unnamed: 0                                                              0\n",
            "function_id                                                         27692\n",
            "signature               private fun bitIndex(elementIndex: Int, bitOff...\n",
            "body                    =\\n        elementIndex * ELEMENT_SIZE + bitOf...\n",
            "is_single_expression                                                 True\n",
            "is_test                                                             False\n",
            "0-20                                                                False\n",
            "100+                                                                False\n",
            "20-50                                                               False\n",
            "50-100                                                               True\n",
            "Name: 0, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python code completion\n",
        "Try few-shot code generation using Phi-1.5, with examples randomly selected from a dataset to provide context\n"
      ],
      "metadata": {
        "id": "5_ro6_ts4Ush"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt_codex(dataset, index, num_examples, context=None, language='Python'):\n",
        "  indices = random.sample(range(len(dataset)), num_examples)\n",
        "  prefix = f'Complete code\\nLanguage: {language}\\n'\n",
        "  shots = '\\n'.join([f\"Example: {dataset[i]['signature']} {dataset[i]['body']}\" for i in indices])\n",
        "\n",
        "  data = dataset[index]\n",
        "  if context:\n",
        "    prompt = f\"{prefix}\\n{shots}\\n{context}\\nCode so far: {data['signature']}\"\n",
        "  else:\n",
        "    prompt = f\"{prefix}\\n{shots}\\nCode so far: {data['signature']}\"\n",
        "\n",
        "  return prompt\n",
        "\n",
        "prompt = create_prompt_codex(codexglue_test, 12, 2)\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False)\n",
        "\n",
        "print(f'\\033[96mFew-shots prompt (without context of functions): \\n{prompt}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eI_rblr02aZc",
        "outputId": "ee15dcc0-e948-454d-ae4f-667addd28b3e"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[96mFew-shots prompt (without context of functions): \n",
            "Complete code\n",
            "Language: Python\n",
            "\n",
            "Example: def _get_name(self): return self.__name \n",
            "Example: @property  def serial_instance(self) -> serial.Serial:  return self._serial_instance \n",
            "Code so far: def loads(s, strip_comments=False, **kw):\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generating output...\")\n",
        "time = datetime.datetime.now()\n",
        "outputs = model.generate(**inputs, max_length=inputs['input_ids'].shape[-1] + 100)\n",
        "text_python = tokenizer.batch_decode(outputs)[0]\n",
        "time1 = datetime.datetime.now()\n",
        "print(f\"Output generated. Time to generate the output: {time1 - time}. \\nOutput:\")\n",
        "print(f'\\033[96m{prompt}\\033[92m{text_python.split(prompt[-10:], 1)[-1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiiDY3ykIbT9",
        "outputId": "5cc3f004-b585-4317-9976-aab17bfdf6f8"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating output...\n",
            "Output generated. Time to generate the output: 0:00:02.956633. \n",
            "Output:\n",
            "\u001b[96mComplete code\n",
            "Language: Python\n",
            "\n",
            "Example: def _get_name(self): return self.__name \n",
            "Example: @property  def serial_instance(self) -> serial.Serial:  return self._serial_instance \n",
            "Code so far: def loads(s, strip_comments=False, **kw):\u001b[92m return json.loads(s, **kw) def dumps(d, **kw): return json.dumps(d, **kw) def _get_name(self): return self.__name \n",
            "def _get_name(self): return self.__name \n",
            "def _get_name(self): return self.__name \n",
            "def _get_name(self): return self.__name \n",
            "def _get_name(self): return self.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's add context of a function"
      ],
      "metadata": {
        "id": "zeUZmtL5S1IZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "function_context = f\"Function context: {codexglue_test[12]['docstring']}\"\n",
        "prompt = create_prompt_codex(codexglue_test, 12, 2, function_context)\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False)\n",
        "print(f'\\033[96mmFew-shots prompt (with context of functions): \\n{prompt}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGTJ2rapS0Ev",
        "outputId": "0e221daa-7b24-4373-ef59-d8d162fe6e75"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[96mmFew-shots prompt (with context of functions): \n",
            "Complete code\n",
            "Language: Python\n",
            "\n",
            "Example: def _set_explicit_path_name(self, v, load=False): if hasattr(v, \"\"):  v = v._utype(v)  try:  t = YANGDynClass( v, base=six.text_type, is_leaf=True, yang_name=\"\", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, namespace=\"\", defining_module=\"\", yang_type=\"\", is_config=True, )  except (TypeError, ValueError):  raise ValueError( { \"\": \"\"\"\"\"\", \"\": \"\", \"\": \"\"\"\"\"\", } )  self.__explicit_path_name = t if hasattr(self, \"\"):  self._set()  \n",
            "Example: async def get_bots(self, limit, offset): if limit > 0:  limit = 50  return await self.request('GET', ''.format(self.BASE, limit, offset)) \n",
            "Function context: Load a list of trees from a Newick formatted string.\n",
            "\n",
            ":param s: Newick formatted string.\n",
            ":param strip_comments: Flag signaling whether to strip comments enclosed in square \\\n",
            "brackets.\n",
            ":param kw: Keyword arguments are passed through to `Node.create`.\n",
            ":return: List of Node objects.\n",
            "Code so far: def loads(s, strip_comments=False, **kw):\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generating output...\")\n",
        "time = datetime.datetime.now()\n",
        "outputs = model.generate(**inputs, max_length=inputs['input_ids'].shape[-1] + 80)\n",
        "text_python_context = tokenizer.batch_decode(outputs)[0]\n",
        "time1 = datetime.datetime.now()\n",
        "print(f\"Output generated. Time to generate the output: {time1 - time}. \\nOutput:\")\n",
        "print(f'\\033[96m{prompt}\\033[92m{text_python_context.split(prompt[-10:], 1)[-1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Wh5kyYnTpwY",
        "outputId": "d58e5336-9804-4a7a-d222-8d13a8ed65b4"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating output...\n",
            "Output generated. Time to generate the output: 0:00:02.370798. \n",
            "Output:\n",
            "\u001b[96mComplete code\n",
            "Language: Python\n",
            "\n",
            "Example: def _set_explicit_path_name(self, v, load=False): if hasattr(v, \"\"):  v = v._utype(v)  try:  t = YANGDynClass( v, base=six.text_type, is_leaf=True, yang_name=\"\", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, namespace=\"\", defining_module=\"\", yang_type=\"\", is_config=True, )  except (TypeError, ValueError):  raise ValueError( { \"\": \"\"\"\"\"\", \"\": \"\", \"\": \"\"\"\"\"\", } )  self.__explicit_path_name = t if hasattr(self, \"\"):  self._set()  \n",
            "Example: async def get_bots(self, limit, offset): if limit > 0:  limit = 50  return await self.request('GET', ''.format(self.BASE, limit, offset)) \n",
            "Function context: Load a list of trees from a Newick formatted string.\n",
            "\n",
            ":param s: Newick formatted string.\n",
            ":param strip_comments: Flag signaling whether to strip comments enclosed in square \\\n",
            "brackets.\n",
            ":param kw: Keyword arguments are passed through to `Node.create`.\n",
            ":return: List of Node objects.\n",
            "Code so far: def loads(s, strip_comments=False, **kw):\u001b[92m\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "import re\n",
            "import sys\n",
            "import os\n",
            "import json\n",
            "import logging\n",
            "import time\n",
            "import datetime\n",
            "import traceback\n",
            "import argparse\n",
            "import requests\n",
            "import yang\n",
            "import yangdyn\n",
            "import yangdyn.utils\n",
            "import yangdyn.exceptions\n",
            "import yangdyn.utils.misc\n",
            "import yangdyn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kotlin code completion\n",
        "With the same methods used as for Python generation (few-shot + context)"
      ],
      "metadata": {
        "id": "v5Kf7yg1VbXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt_kotlin(dataset, index, num_examples, context=None, language='Kotlin'):\n",
        "  indices = random.sample(range(len(dataset)), num_examples)\n",
        "  prefix = f'Complete code\\nLanguage: {language}\\n'\n",
        "  shots = '\\n'.join([f\"Example: {dataset.iloc[i]['signature']} {dataset.iloc[i]['body']}\" for i in indices])\n",
        "\n",
        "  data = dataset.iloc[index]\n",
        "  if context:\n",
        "    prompt = f\"{prefix}\\n{shots}\\n{context}\\nCode so far: {data['signature']}\"\n",
        "  else:\n",
        "    prompt = f\"{prefix}\\n{shots}\\nCode so far: {data['signature']}\"\n",
        "\n",
        "  return prompt\n",
        "\n",
        "prompt = create_prompt_kotlin(functions_df, 12, 2)\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False)\n",
        "\n",
        "print(f'\\033[96mFew-shots prompt (without context of functions): \\n{prompt}\\n')"
      ],
      "metadata": {
        "id": "rm-bMXiUlsxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5509db31-5908-4dda-9e25-fad46d2e64dc"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[96mFew-shots prompt (without context of functions): \n",
            "Complete code\n",
            "Language: Kotlin\n",
            "\n",
            "Example: fun unzipTo(destinationDirectory: File, fromSubdirectory: File = File(\"/\"), resetTimeAttributes: Boolean = false): File {\n",
            "    withZipFileSystem {\n",
            "        it.file(fromSubdirectory).recursiveCopyTo(destinationDirectory, resetTimeAttributes)\n",
            "    }\n",
            "}\n",
            "Example: fun foo() {\n",
            "    var x: (@[Foo Bar] (()->Unit)-> ()->Unit) -> Unit = {}\n",
            "}\n",
            "Code so far: fun assertTasksPackedToCache(vararg taskPaths: String): BuildResult\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generating output...\")\n",
        "time = datetime.datetime.now()\n",
        "outputs = model.generate(**inputs, max_length=inputs['input_ids'].shape[-1] + 80)\n",
        "text_kotlin = tokenizer.batch_decode(outputs)[0]\n",
        "time1 = datetime.datetime.now()\n",
        "print(f\"Output generated. Time to generate the output: {time1 - time}. \\nOutput:\")\n",
        "print(f'\\033[96m{prompt}\\033[92m{text_kotlin.split(prompt[-10:], 1)[-1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_z-aw2NYSMk",
        "outputId": "5fb9d1f1-75aa-47d2-aaa1-9ffed18539be"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating output...\n",
            "Output generated. Time to generate the output: 0:00:02.384135. \n",
            "Output:\n",
            "\u001b[96mComplete code\n",
            "Language: Kotlin\n",
            "\n",
            "Example: fun unzipTo(destinationDirectory: File, fromSubdirectory: File = File(\"/\"), resetTimeAttributes: Boolean = false): File {\n",
            "    withZipFileSystem {\n",
            "        it.file(fromSubdirectory).recursiveCopyTo(destinationDirectory, resetTimeAttributes)\n",
            "    }\n",
            "}\n",
            "Example: fun foo() {\n",
            "    var x: (@[Foo Bar] (()->Unit)-> ()->Unit) -> Unit = {}\n",
            "}\n",
            "Code so far: fun assertTasksPackedToCache(vararg taskPaths: String): BuildResult\u001b[92m {\n",
            "    val taskPaths = taskPaths.toList()\n",
            "    val taskPathsWithCache = taskPaths.map { path -> path.withFile(path.withName(\".cache\")) }\n",
            "    val taskPathsWithCacheAndCache = taskPathsWithCache.map { path -> path.withFile(path.withName(\".cache\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT-generated\n",
        "function_context = '''Function context: The assertTasksPackedToCache function verifies that cache entries have been stored for\n",
        "the provided task paths by iterating through each path and asserting that the output contains\n",
        "a specific message indicating the cache entry\\'s storage.'''\n",
        "\n",
        "prompt = create_prompt_kotlin(functions_df, 12, 2, function_context)\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False)\n",
        "print(f'\\033[96mFew-shots prompt (with context of functions): \\n{prompt}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bj-f8VRoDCca",
        "outputId": "037124ce-3fb7-4137-c2ac-30024e51ae03"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[96mFew-shots prompt (with context of functions): \n",
            "Complete code\n",
            "Language: Kotlin\n",
            "\n",
            "Example: fun case_6(x: Any?) {\n",
            "    if (x !is String?) throw Exception()\n",
            "    <!DEBUG_INFO_EXPRESSION_TYPE(\"kotlin.Any? & kotlin.String?\")!>x<!>\n",
            "    <!DEBUG_INFO_EXPRESSION_TYPE(\"kotlin.Any? & kotlin.String?\"), DEBUG_INFO_SMARTCAST!>x<!>?.length\n",
            "}\n",
            "Example: override fun getType(expression: KtExpression) {\n",
            "            return this@DelegatingBindingTrace.getType(expression)\n",
            "        }\n",
            "Function context: The assertTasksPackedToCache function verifies that cache entries have been stored for \n",
            "the provided task paths by iterating through each path and asserting that the output contains \n",
            "a specific message indicating the cache entry's storage.\n",
            "Code so far: fun assertTasksPackedToCache(vararg taskPaths: String): BuildResult\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generating output...\")\n",
        "time = datetime.datetime.now()\n",
        "outputs = model.generate(**inputs, max_length=inputs['input_ids'].shape[-1] + 80)\n",
        "text_kotlin_context = tokenizer.batch_decode(outputs)[0]\n",
        "time1 = datetime.datetime.now()\n",
        "print(f\"Output generated. Time to generate the output: {time1 - time}. \\nOutput:\")\n",
        "\n",
        "print(f'\\033[96m{prompt}\\033[92m {text_kotlin_context.split(prompt[-10:], 1)[-1]}')"
      ],
      "metadata": {
        "id": "o2s8H4PBGXGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbb02582-f9b9-4bd1-e476-a3fdb2f5264c"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating output...\n",
            "Output generated. Time to generate the output: 0:00:02.402167. \n",
            "Output:\n",
            "\u001b[96mComplete code\n",
            "Language: Kotlin\n",
            "\n",
            "Example: fun case_6(x: Any?) {\n",
            "    if (x !is String?) throw Exception()\n",
            "    <!DEBUG_INFO_EXPRESSION_TYPE(\"kotlin.Any? & kotlin.String?\")!>x<!>\n",
            "    <!DEBUG_INFO_EXPRESSION_TYPE(\"kotlin.Any? & kotlin.String?\"), DEBUG_INFO_SMARTCAST!>x<!>?.length\n",
            "}\n",
            "Example: override fun getType(expression: KtExpression) {\n",
            "            return this@DelegatingBindingTrace.getType(expression)\n",
            "        }\n",
            "Function context: The assertTasksPackedToCache function verifies that cache entries have been stored for \n",
            "the provided task paths by iterating through each path and asserting that the output contains \n",
            "a specific message indicating the cache entry's storage.\n",
            "Code so far: fun assertTasksPackedToCache(vararg taskPaths: String): BuildResult\u001b[92m <String, BuildResult<String, BuildResult<String, BuildResult<String, BuildResult<String, BuildResult<String, BuildResult<String, BuildResult<String, BuildResult<String, BuildResult<String, BuildResult<String, BuildResult<String, BuildResult<String, BuildResult<String, BuildResult<String, BuildResult<String, BuildResult\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# And the real answer is\n",
        "example = functions_df.iloc[12]\n",
        "print(example['signature'] + example['body'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSgELalkZEO8",
        "outputId": "bb6740e9-1e39-4e03-e332-c6b2c7ed71dd"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fun assertTasksPackedToCache(vararg taskPaths: String): BuildResult{\n",
            "    taskPaths.forEach {\n",
            "        assertOutputContains(\"Stored cache entry for task '$it' with cache key \")\n",
            "    }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# LLM output control\n",
        "Since we can see that the model generates unnecessary parts and repeats some code, we can try to use some techniques for preventing it\n"
      ],
      "metadata": {
        "id": "5E51G_9B_ii_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take the last prompt\n",
        "prompt = prompt\n",
        "print(\"Prompt:\")\n",
        "print(prompt)\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False)\n",
        "\n",
        "# Top-k Sampling: Sampling from the top k most likely next words\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_length=inputs['input_ids'].shape[-1] + 80,\n",
        "    do_sample=True,\n",
        "    top_k=50\n",
        ")\n",
        "text_top_k = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "print(\"\\nGenerated text with Top-k Sampling:\")\n",
        "print(f'\\033[92m {text_top_k.split(prompt[-10:], 1)[-1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk5ohrQY6Gmg",
        "outputId": "26bf544b-c0f1-43bf-b0b8-b92fbdeccf7e"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n",
            "Complete code\n",
            "Language: Kotlin\n",
            "\n",
            "Example: fun case_6(x: Any?) {\n",
            "    if (x !is String?) throw Exception()\n",
            "    <!DEBUG_INFO_EXPRESSION_TYPE(\"kotlin.Any? & kotlin.String?\")!>x<!>\n",
            "    <!DEBUG_INFO_EXPRESSION_TYPE(\"kotlin.Any? & kotlin.String?\"), DEBUG_INFO_SMARTCAST!>x<!>?.length\n",
            "}\n",
            "Example: override fun getType(expression: KtExpression) {\n",
            "            return this@DelegatingBindingTrace.getType(expression)\n",
            "        }\n",
            "Function context: The assertTasksPackedToCache function verifies that cache entries have been stored for \n",
            "the provided task paths by iterating through each path and asserting that the output contains \n",
            "a specific message indicating the cache entry's storage.\n",
            "Code so far: fun assertTasksPackedToCache(vararg taskPaths: String): BuildResult\n",
            "\n",
            "Generated text with Top-k Sampling:\n",
            "\u001b[92m <Int, Int, BuildContext<String>\n",
            "        if (taskPaths.size == 0) {\n",
            "            // nothing to do\n",
            "        } else {\n",
            "            var output: BuildContext<String> = BuildResult(\n",
            "                1,\n",
            "                0,\n",
            "                buildContextClass(\n",
            "                    taskContextClass(\n",
            "                        taskContextName(taskPaths.joinTo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-p (Nucleus) Sampling: Sampling from a dynamically adjusted set of words\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_length=inputs['input_ids'].shape[-1] + 80,\n",
        "    do_sample=True,\n",
        "    top_p=0.9\n",
        ")\n",
        "text_top_p = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "print(\"\\nGenerated text with Top-p (Nucleus) Sampling:\")\n",
        "print(f'\\033[92m {text_top_p.split(prompt[-10:], 1)[-1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZhDO6LaAvtW",
        "outputId": "def02998-36e5-40b3-c3a7-96fb0f0f4e42"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated text with Top-p (Nucleus) Sampling:\n",
            "\u001b[92m <String, BuildResult<List<Dictionary<String, Object>>, BuildResult<List<Dictionary<String, Object>>>>, BuildResult<List<Dictionary<String, Object>>>>, BuildResult<List<Dictionary<String, Object>>>, BuildResult<List<Dictionary<String, Object>>>> {\n",
            "        taskPaths.forEach(task\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Temperature Scaling: Adjusting the likelihood scores of words before sampling\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_length=inputs['input_ids'].shape[-1] + 80,\n",
        "    do_sample=True,\n",
        "    temperature=0.7\n",
        ")\n",
        "text_temp = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "print(\"\\nGenerated text with Temperature Scaling:\")\n",
        "print(f'\\033[92m {text_temp.split(prompt[-10:], 1)[-1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJKu2qolAxKj",
        "outputId": "765e21a5-885a-4c2e-b35a-0f24a9055318"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated text with Temperature Scaling:\n",
            "\u001b[92m  {\n",
            "    for (taskPath in taskPaths) {\n",
            "        var cacheEntry = BuildCache.get(taskPath)\n",
            "        assertTasksPackedToCache(taskPath, cacheEntry)\n",
            "    }\n",
            "}\n",
            "Output: Function trace:\n",
            "            Assertion failed at line 3.\n",
            "            BuildCache did not store cache entry for task path '$taskPath\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Repetition Penalty: Penalizing repeated tokens in the generated output\n",
        "repetition_penalty = 1.0\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_length=inputs['input_ids'].shape[-1] + 80,\n",
        "    do_sample=True,\n",
        "    repetition_penalty=repetition_penalty\n",
        ")\n",
        "text_rep_pen = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "print(\"\\nGenerated text with Repetition Penalty:\")\n",
        "print(f'\\033[92m {text_rep_pen.split(prompt[-10:], 1)[-1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Efi261ZAyMI",
        "outputId": "5375cc7b-cad8-476c-8762-313de3b514ce"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated text with Repetition Penalty:\n",
            "\u001b[92m ? {\n",
            "    assertBuildResult(true)\n",
            "    taskPaths.forEach { taskPath ->\n",
            "        try {\n",
            "            var cacheResults = taskCache.get(taskPath)\n",
            "\n",
            "            if (cacheResults!= null && cacheResults.map { cacheEntry -> CacheEntryPair<KtExpression, BuildResult>() }) {\n",
            "\n",
            "                val cacheEntryP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will search best combinations of these parameters later"
      ],
      "metadata": {
        "id": "hijk6Hn0ES4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output evaluation\n",
        "Generate output for some small sample from CodeXGlue and compute few metrics"
      ],
      "metadata": {
        "id": "VjTulUspZ_KA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = codexglue_test[1]\n",
        "function_context = f\"Function context: {example['docstring']}\"\n",
        "prompt = create_prompt_codex(codexglue_test, 1, 2, function_context)\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False)\n",
        "print(f'Few-shots prompt: \\n\\033[96m{prompt}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTdRISqFaBPc",
        "outputId": "5b81a3dd-f1fa-498e-bc6c-ef33bfd07e9c"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Few-shots prompt: \n",
            "\u001b[96mComplete code\n",
            "Language: Python\n",
            "\n",
            "Example: def _get_router_id(self): return self.__router_id \n",
            "Example: def bring_gpio_interrupt_into_userspace():   try:  with open(GPIO_INTERRUPT_DEVICE_VALUE):  return   except IOError:  with open(GPIO_EXPORT_FILE, 'w') as export_file:  export_file.write(str(GPIO_INTERRUPT_PIN))  wait_until_file_exists(GPIO_INTERRUPT_DEVICE_VALUE)  \n",
            "Function context: Adds a message with the ``SUCCESS`` level.\n",
            "\n",
            ":param user: User instance\n",
            ":param message: Message to show\n",
            "Code so far: def success(user, message):\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_length=inputs['input_ids'].shape[-1] + 5,\n",
        ")\n",
        "text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "text = text.split(prompt[-10:], 1)[-1]\n",
        "print(\"\\nGenerated text:\")\n",
        "print(f'\\033[92m {text}')\n",
        "print(\"\\n\\033[0mReal code:\")\n",
        "print(f'\\033[92m {example[\"body\"]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOkkV69MFwGF",
        "outputId": "357edc53-5f59-4b57-a80b-652987e90891"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated text:\n",
            "\u001b[92m  print(message)\n",
            "\n",
            "\n",
            "\u001b[0mReal code:\n",
            "\u001b[92m message_user(user, message, constants.SUCCESS) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute metrics\n",
        "EM = 0.0\n",
        "edit_sim = 0.0\n",
        "\n",
        "pred = text\n",
        "gt = example[\"body\"]\n",
        "print(f'Edit sim: {fuzz.ratio(pred, gt)}')\n",
        "print(f'Exact match: {int(pred.split() == gt.split())}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tGHR8d6SmFd",
        "outputId": "88d99468-8fc4-4b30-f692-fad27889f446"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Edit sim: 25\n",
            "Exact match: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def flex_accuracy(s1, s2, tolerance=None):\n",
        "    \"\"\"\n",
        "    Compute the flex accuracy between two strings with a given tolerance.\n",
        "    \"\"\"\n",
        "    distance = Levenshtein.distance(s1, s2)\n",
        "    max_length = max(len(s1), len(s2))\n",
        "    similarity = 1 - (distance / max_length)\n",
        "    if tolerance:\n",
        "      if similarity >= tolerance:\n",
        "        return similarity\n",
        "      else:\n",
        "          return 0\n",
        "    else:\n",
        "      return similarity\n",
        "\n",
        "# Example usage:\n",
        "tolerance = 0.8\n",
        "accuracy = flex_accuracy(pred, gt)\n",
        "print(\"Flex accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Sbb1zQtT5Ra",
        "outputId": "019b0343-71c3-4031-9a86-c11d31c4f387"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flex accuracy: 0.19148936170212771\n"
          ]
        }
      ]
    }
  ]
}